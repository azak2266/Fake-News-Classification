{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8ac0957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#將資料讀取\n",
    "import pandas as pd\n",
    "TRAIN_CSV_PATH = 'train.csv'\n",
    "train = pd.read_csv(TRAIN_CSV_PATH, index_col=0,encoding='utf8')\n",
    "cols = ['title1_zh', 'title2_zh', 'label']\n",
    "train = train.loc[:, cols]\n",
    "train = train.dropna()#去除空值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "91761ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>label</th>\n",
       "      <th>title1_tokenizer</th>\n",
       "      <th>title2_tokenizer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗</td>\n",
       "      <td>警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "      <td>深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "      <td>GDP 首 超 香港 深圳 澄清 还 差 一点点</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title1_zh                  title2_zh      label  \\\n",
       "id                                                                            \n",
       "0       2017养老保险又新增两项，农村老人人人可申领，你领到了吗   警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京  unrelated   \n",
       "3   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小  unrelated   \n",
       "1   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港       GDP首超香港？深圳澄清：还差一点点……  unrelated   \n",
       "\n",
       "                                   title1_tokenizer  \\\n",
       "id                                                    \n",
       "0          2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗   \n",
       "3   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港   \n",
       "1   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港   \n",
       "\n",
       "                      title2_tokenizer  \n",
       "id                                      \n",
       "0    警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京  \n",
       "3   深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小  \n",
       "1             GDP 首 超 香港 深圳 澄清 还 差 一点点  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "\n",
    "def jieba_tokenizer(text):\n",
    "    words = pseg.cut(text)\n",
    "    return ' '.join([word for word, flag in words if flag != 'x'])\n",
    "\n",
    "\n",
    "#要做斷詞，將其中一列分出來\n",
    "train['title1_tokenizer'] = train.loc[:,'title1_zh'].apply(jieba_tokenizer)\n",
    "train['title2_tokenizer'] = train.loc[:,'title2_zh'].apply(jieba_tokenizer)\n",
    "train.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "15986c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#斷詞太久了，先將檔案存起來\n",
    "train.to_csv('output.csv', encoding = 'utf-8-sig') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "277a9772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[217, 1268, 32, 1178, 5967, 25, 489, 2877, 116, 5559, 4, 1850, 2, 13]]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "MAX_NUM_WORDS = 10000\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "\n",
    "#製作字典\n",
    "corpus1 = train.title1_tokenizer#新聞A\n",
    "corpus2 = train.title2_tokenizer#新聞B\n",
    "corpus = pd.concat([corpus1,corpus2])#合再一起\n",
    "\n",
    "#丟進分詞器\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "Anews_train = tokenizer.texts_to_sequences(corpus1)\n",
    "Bnews_train = tokenizer.texts_to_sequences(corpus2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6a6f7045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[217, 1268, 32, 1178, 5967, 25, 489, 2877, 116, 5559, 4, 1850, 2, 13]]\n",
      "['2017', '养老保险', '又', '新增', '两项', '农村', '老人', '人人', '可', '申领', '你', '领到', '了', '吗']\n"
     ]
    }
   ],
   "source": [
    "#檢查中文數字轉換\n",
    "print(Anews_train[:1])\n",
    "for seq in Anews_train[:1]:\n",
    "    print([tokenizer.index_word[idx] for idx in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6622d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#因為句子的長度不同，所以要做padding\n",
    "max_len = 15#設定最大長度，不夠補0超過去掉。\n",
    "Anews_train = keras.preprocessing.sequence.pad_sequences(Anews_train,max_len)\n",
    "Bnews_train = keras.preprocessing.sequence.pad_sequences(Bnews_train,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "624dcb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label\n",
    "import numpy as np\n",
    "label_to_index = {'unrelated': 0, 'agreed': 1, 'disagreed': 2}\n",
    "y_train = train.label.apply(lambda x: label_to_index[x])\n",
    "y_train = np.asarray(y_train).astype('float32')\n",
    "y_train =  keras.utils.to_categorical(y_train)\n",
    "y_train[:5]\n",
    "\n",
    "#製作訓練、驗證資料集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "x1_train, x1_val, x2_train, x2_val, y_train, y_val = train_test_split(Anews_train, Bnews_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "04f6f7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 15, 256)      2560000     input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 128)          197120      embedding_4[0][0]                \n",
      "                                                                 embedding_4[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 256)          0           lstm_4[0][0]                     \n",
      "                                                                 lstm_4[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            771         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,757,891\n",
      "Trainable params: 2,757,891\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Embedding, LSTM, concatenate, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "#tensorflow version 2.3\n",
    "\n",
    "\n",
    "#Dataset\n",
    "seq_lenght = 15\n",
    "anews_input = Input(shape=(seq_lenght, ),dtype = 'int32')\n",
    "bnews_input = Input(shape=(seq_lenght, ),dtype = 'int32')\n",
    "\n",
    "#Embedding\n",
    "embedding_layer = Embedding(10000,256)\n",
    "anews_mebedding = embedding_layer(anews_input)\n",
    "bnews_mebedding = embedding_layer(bnews_input)\n",
    "\n",
    "#LSTM\n",
    "share_Lstm = LSTM(128)\n",
    "anews_lstm = share_Lstm(anews_mebedding)\n",
    "bnews_lstm = share_Lstm(bnews_mebedding)\n",
    "\n",
    "#Concat\n",
    "concat = concatenate([anews_lstm,bnews_lstm], axis = -1)\n",
    "\n",
    "#Dense\n",
    "dense = Dense(units=3, activation = 'softmax')\n",
    "predictions = dense(concat)\n",
    "\n",
    "#Model\n",
    "model = Model(inputs = [anews_input,bnews_input], outputs = predictions)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6a16e3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "564/564 [==============================] - 47s 84ms/step - loss: 0.1364 - accuracy: 0.9497 - val_loss: 0.4379 - val_accuracy: 0.8501\n",
      "Epoch 2/10\n",
      "564/564 [==============================] - 48s 84ms/step - loss: 0.1229 - accuracy: 0.9549 - val_loss: 0.4591 - val_accuracy: 0.8502\n",
      "Epoch 3/10\n",
      "564/564 [==============================] - 48s 85ms/step - loss: 0.1153 - accuracy: 0.9581 - val_loss: 0.4766 - val_accuracy: 0.8505\n",
      "Epoch 4/10\n",
      "564/564 [==============================] - 48s 84ms/step - loss: 0.1095 - accuracy: 0.9602 - val_loss: 0.5034 - val_accuracy: 0.8525\n",
      "Epoch 5/10\n",
      "564/564 [==============================] - 48s 84ms/step - loss: 0.1051 - accuracy: 0.9618 - val_loss: 0.4964 - val_accuracy: 0.8472\n",
      "Epoch 6/10\n",
      "564/564 [==============================] - 48s 84ms/step - loss: 0.1007 - accuracy: 0.9637 - val_loss: 0.5089 - val_accuracy: 0.8505\n",
      "Epoch 7/10\n",
      "564/564 [==============================] - 48s 84ms/step - loss: 0.0976 - accuracy: 0.9650 - val_loss: 0.5288 - val_accuracy: 0.8528\n",
      "Epoch 8/10\n",
      "564/564 [==============================] - 48s 84ms/step - loss: 0.0944 - accuracy: 0.9660 - val_loss: 0.5284 - val_accuracy: 0.8469\n",
      "Epoch 9/10\n",
      "564/564 [==============================] - 48s 84ms/step - loss: 0.0918 - accuracy: 0.9671 - val_loss: 0.5503 - val_accuracy: 0.8502\n",
      "Epoch 10/10\n",
      "564/564 [==============================] - 48s 84ms/step - loss: 0.0895 - accuracy: 0.9679 - val_loss: 0.5380 - val_accuracy: 0.8484\n"
     ]
    }
   ],
   "source": [
    "#Fit model\n",
    "result = model.fit(x = [x1_train, x2_train], y = y_train, batch_size=512, epochs=10, validation_data=([x1_val, x2_val], y_val),shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "94d9cb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zn/miniconda3/envs/test_lstm/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/zn/miniconda3/envs/test_lstm/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: lstm_fakenews/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('lstm_fakenews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "48a2c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testDataset\n",
    "import pandas as pd\n",
    "test = pd.read_csv('test.csv', index_col=0,encoding='utf8')\n",
    "test = test.dropna()\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "def jieba_tokenizer(text):\n",
    "    words = pseg.cut(text)\n",
    "    return ' '.join([word for word, flag in words if flag != 'x'])\n",
    "\n",
    "#seg\n",
    "test['title1_tokenizer'] = test.title1_zh.apply(jieba_tokenizer)\n",
    "test['title2_tokenizer'] = test.title2_zh.apply(jieba_tokenizer)\n",
    "\n",
    "#seq2int\n",
    "x1_test = tokenizer.sequences_to_texts(test['title1_tokenizer'])\n",
    "x2_test = tokenizer.sequences_to_texts(test['title2_tokenizer'])\n",
    "\n",
    "max_len = 15\n",
    "x1_test = keras.preprocessing.sequence.pad_sequences(x1_test,maxlen=max_len)\n",
    "x2_test = keras.preprocessing.sequence.pad_sequences(x2_test,maxlen=max_len)\n",
    "\n",
    "predictions = model.predict([x1_test,x2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f1fb572c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
